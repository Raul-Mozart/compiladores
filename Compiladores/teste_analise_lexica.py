"""
SISTEMA DE TESTE DE AN√ÅLISE L√âXICA
Integrado com os aut√¥matos existentes para testar a linguagem proposta

Este m√≥dulo permite:
1. Testar tokens individuais usando os aut√¥matos DFA
2. Processar arquivos de c√≥digo na linguagem proposta
3. Avaliar input do usu√°rio interativamente
4. Gerar relat√≥rios detalhados de an√°lise l√©xica
"""

import re
import os
from typing import List, Dict, Tuple, Optional
from automatos import compile_regex_to_dfa
from automatos.constantes import TOKENS_COMUNS


class AnalisadorLexico:
    """Analisador l√©xico para a linguagem proposta"""
    
    def __init__(self):
        """Inicializa o analisador com tokens da linguagem proposta"""
        # Tokens espec√≠ficos da linguagem (baseados na especifica√ß√£o GitHub)
        self.tokens_linguagem = {
            # Palavras-chave
            "KEYWORD": r"(if|else|for|while|function|var|in|class|return|string|int|float|bool|list|and|or|not|private|public|mutable|inherits|new)",
            
            # Identificadores (mais espec√≠fico que o gen√©rico)
            "IDENTIFIER": r"[A-Za-z_][A-Za-z0-9_]*",
            
            # Literais num√©ricos
            "INT_LITERAL": r"\d+",
            "FLOAT_LITERAL": r"\d+\.\d+([eE][+-]?\d+)?",
            
            # Literais de string (com escape)
            "STRING_LITERAL": r'"([^"\\]|\\[\\"])*"',
            
            # Literais booleanos
            "BOOL_LITERAL": r"(true|false)",
            
            # Operadores relacionais
            "RELOP": r"(==|!=|<=|>=|<|>)",
            
            # Operadores aritm√©ticos
            "ARITHOP": r"[+\-*/%]",
            
            # Operador de atribui√ß√£o
            "ASSIGN": r"=",
            
            # Delimitadores
            "LPAREN": r"\(",
            "RPAREN": r"\)",
            "LBRACE": r"\{",
            "RBRACE": r"\}",
            "LBRACKET": r"\[",
            "RBRACKET": r"\]",
            
            # Pontua√ß√£o
            "SEMICOLON": r";",
            "COMMA": r",",
            "DOT": r"\.",
            "COLON": r":",
            
            # Quebra de linha e espa√ßos
            "NEWLINE": r"\n",
            "WHITESPACE": r"[ \t\r]+",
            
            # Coment√°rios (adicionando suporte)
            "COMMENT": r"//[^\n]*",
            
            # Operador de range
            "RANGE": r"\.\."
        }
        
        # Compilar todos os tokens para DFA
        self.dfas_compilados = {}
        self.compilar_tokens()
    
    def compilar_tokens(self):
        """Compila todos os tokens para aut√¥matos DFA"""
        print("üîÑ Compilando tokens para aut√¥matos DFA...")
        
        for nome, regex in self.tokens_linguagem.items():
            try:
                dfa = compile_regex_to_dfa(regex)
                self.dfas_compilados[nome] = dfa
                print(f"  ‚úÖ {nome}: {len(dfa.states)} estados")
            except Exception as e:
                print(f"  ‚ùå {nome}: ERRO ‚Üí {e}")
        
        print(f"üìä Total de tokens compilados: {len(self.dfas_compilados)}")
    
    def tokenizar_codigo(self, codigo: str) -> List[Tuple[str, str, int, int]]:
        """
        Tokeniza c√≥digo fonte da linguagem
        
        Returns:
            Lista de (token_type, lexeme, linha, coluna)
        """
        tokens_encontrados = []
        linha_atual = 1
        coluna_atual = 1
        posicao = 0
        
        while posicao < len(codigo):
            token_encontrado = False
            
            # Tenta encontrar o maior match poss√≠vel
            melhor_match = None
            maior_comprimento = 0
            
            for nome_token, dfa in self.dfas_compilados.items():
                # Testa substring a partir da posi√ß√£o atual
                for fim in range(posicao + 1, len(codigo) + 1):
                    substring = codigo[posicao:fim]
                    
                    if dfa.accepts(substring):
                        if len(substring) > maior_comprimento:
                            maior_comprimento = len(substring)
                            melhor_match = (nome_token, substring)
                    else:
                        # Se n√£o aceita, n√£o precisa testar strings maiores
                        break
            
            if melhor_match:
                token_type, lexeme = melhor_match
                
                # Pula whitespace e coment√°rios na sa√≠da (mas ainda processa)
                if token_type not in ["WHITESPACE", "COMMENT"]:
                    tokens_encontrados.append((token_type, lexeme, linha_atual, coluna_atual))
                
                # Atualiza posi√ß√£o
                posicao += len(lexeme)
                
                # Atualiza linha e coluna
                if token_type == "NEWLINE" or '\n' in lexeme:
                    linha_atual += lexeme.count('\n')
                    coluna_atual = 1
                else:
                    coluna_atual += len(lexeme)
                
                token_encontrado = True
            
            if not token_encontrado:
                # Caractere n√£o reconhecido
                char = codigo[posicao]
                tokens_encontrados.append(("UNKNOWN", char, linha_atual, coluna_atual))
                posicao += 1
                coluna_atual += 1
        
        return tokens_encontrados
    
    def validar_token_individual(self, token_type: str, lexeme: str) -> bool:
        """Valida se um lexeme corresponde a um tipo de token espec√≠fico"""
        if token_type in self.dfas_compilados:
            return self.dfas_compilados[token_type].accepts(lexeme)
        return False
    
    def processar_arquivo(self, caminho_arquivo: str) -> Dict:
        """Processa arquivo de c√≥digo e retorna an√°lise completa"""
        if not os.path.exists(caminho_arquivo):
            return {"erro": f"Arquivo n√£o encontrado: {caminho_arquivo}"}
        
        try:
            with open(caminho_arquivo, 'r', encoding='utf-8') as arquivo:
                codigo = arquivo.read()
            
            tokens = self.tokenizar_codigo(codigo)
            
            # Estat√≠sticas
            stats = {}
            for token_type, _, _, _ in tokens:
                stats[token_type] = stats.get(token_type, 0) + 1
            
            return {
                "arquivo": caminho_arquivo,
                "tokens": tokens,
                "estatisticas": stats,
                "total_tokens": len(tokens),
                "linhas_codigo": codigo.count('\n') + 1
            }
        
        except Exception as e:
            return {"erro": f"Erro ao processar arquivo: {e}"}
    
    def gerar_relatorio(self, resultado: Dict) -> str:
        """Gera relat√≥rio detalhado da an√°lise"""
        if "erro" in resultado:
            return f"‚ùå ERRO: {resultado['erro']}"
        
        relatorio = []
        relatorio.append("=" * 80)
        relatorio.append("üìã RELAT√ìRIO DE AN√ÅLISE L√âXICA")
        relatorio.append("=" * 80)
        relatorio.append(f"üìÅ Arquivo: {resultado['arquivo']}")
        relatorio.append(f"üìä Total de tokens: {resultado['total_tokens']}")
        relatorio.append(f"üìù Linhas de c√≥digo: {resultado['linhas_codigo']}")
        relatorio.append("")
        
        # Estat√≠sticas por tipo
        relatorio.append("üìà ESTAT√çSTICAS POR TIPO DE TOKEN:")
        relatorio.append("-" * 50)
        for token_type, count in sorted(resultado['estatisticas'].items()):
            relatorio.append(f"  {token_type:15}: {count:4d} ocorr√™ncias")
        
        relatorio.append("")
        
        # Todos os tokens encontrados
        relatorio.append("üîç TOKENS ENCONTRADOS:")
        relatorio.append("-" * 50)
        relatorio.append(f"{'LINHA':>5} {'COL':>4} {'TIPO':15} {'LEXEME'}")
        relatorio.append("-" * 50)
        
        for token_type, lexeme, linha, coluna in resultado['tokens']:
            # Escape strings para display
            if token_type == "STRING_LITERAL":
                lexeme_display = repr(lexeme)
            else:
                lexeme_display = lexeme.replace('\n', '\\n').replace('\t', '\\t')
            
            relatorio.append(f"{linha:5d} {coluna:4d} {token_type:15} {lexeme_display}")
        
        return "\n".join(relatorio)


def testar_tokens_individuais():
    """Testa tokens individuais usando os aut√¥matos"""
    print("\n" + "=" * 60)
    print("üß™ TESTANDO TOKENS INDIVIDUAIS")
    print("=" * 60)
    
    analisador = AnalisadorLexico()
    
    casos_teste = {
        "KEYWORD": ["if", "else", "function", "var", "while", "for", "invalid_keyword"],
        "IDENTIFIER": ["variavel", "_private", "contador1", "9invalid", "valid_name"],
        "INT_LITERAL": ["123", "0", "007", "12.5", "abc"],
        "FLOAT_LITERAL": ["3.14", "0.0", "1.5e10", "123", ".5"],
        "STRING_LITERAL": ['"hello"', '"world \\"quoted\\""', '""', '"unterminated', '"valid"'],
        "BOOL_LITERAL": ["true", "false", "True", "FALSE", "maybe"],
        "RELOP": ["==", "!=", "<=", ">=", "<", ">", "==="],
        "ARITHOP": ["+", "-", "*", "/", "%", "++", "--"],
    }
    
    for token_type, exemplos in casos_teste.items():
        print(f"\nüè∑Ô∏è  {token_type}:")
        for exemplo in exemplos:
            valido = analisador.validar_token_individual(token_type, exemplo)
            status = "‚úÖ" if valido else "‚ùå"
            print(f"  {status} '{exemplo}'")


def criar_arquivo_exemplo():
    """Cria arquivo de exemplo com c√≥digo na linguagem proposta"""
    codigo_exemplo = '''// PROGRAMA DE EXEMPLO - Linguagem Proposta
// Testando todos os tipos de tokens

var int idade as 25;
var float altura as 1.75;
var string nome as "Jo√£o Silva";
var bool ativo as true;
var list numeros as [1, 2, 3, 4, 5];

function string verificarIdade(idade) {
    if (idade >= 18) {
        return "Maior de idade";
    } else if (idade >= 13) {
        return "Adolescente";  
    } else {
        return "Crian√ßa";
    }
}

function float calcularMedia(a, b, c) {
    var float soma as a + b + c;
    var float media as soma / 3.0;
    return media;
}

class Pessoa {
    var string nome as "";
    var int idade as 0;
    
    function void setNome(novoNome) {
        nome = novoNome;
    }
}
return

// Programa principal
var int numeroA as 10;
var int numeroB as 20; 
var float resultado as calcularMedia(numeroA, numeroB, 15);

if (resultado > 15.0 and numeroA != numeroB) {
    print("Resultado v√°lido: " + resultado);
} else {
    print("Resultado inv√°lido");
}

for int i in 1 .. 5 {
    print("N√∫mero: " + i);
}
'''
    
    caminho = "codigo_exemplo.txt"
    with open(caminho, 'w', encoding='utf-8') as arquivo:
        arquivo.write(codigo_exemplo)
    
    return caminho


def menu_interativo():
    """Menu interativo para o usu√°rio testar o analisador"""
    analisador = AnalisadorLexico()
    
    while True:
        print("\n" + "=" * 60)
        print("üî¨ ANALISADOR L√âXICO - MENU INTERATIVO")
        print("=" * 60)
        print("1. Testar arquivo de c√≥digo")
        print("2. Testar c√≥digo digitado")
        print("3. Criar arquivo de exemplo")
        print("4. Testar tokens individuais")
        print("5. Sair")
        
        opcao = input("\nüìù Escolha uma op√ß√£o (1-5): ").strip()
        
        if opcao == "1":
            caminho = input("üìÅ Digite o caminho do arquivo: ").strip()
            if not caminho:
                caminho = "codigo_exemplo.txt"
                print(f"üìã Usando arquivo padr√£o: {caminho}")
            
            resultado = analisador.processar_arquivo(caminho)
            relatorio = analisador.gerar_relatorio(resultado)
            print("\n" + relatorio)
            
            # Salvar relat√≥rio
            with open("relatorio_analise.txt", "w", encoding="utf-8") as f:
                f.write(relatorio)
            print(f"\nüíæ Relat√≥rio salvo em: relatorio_analise.txt")
        
        elif opcao == "2":
            print("üìù Digite o c√≥digo (termine com linha vazia):")
            linhas = []
            while True:
                linha = input()
                if linha.strip() == "":
                    break
                linhas.append(linha)
            
            codigo = "\n".join(linhas)
            if codigo.strip():
                tokens = analisador.tokenizar_codigo(codigo)
                
                print("\nüîç TOKENS ENCONTRADOS:")
                print("-" * 50)
                for token_type, lexeme, linha, coluna in tokens:
                    print(f"{linha:3d}:{coluna:3d} {token_type:15} '{lexeme}'")
        
        elif opcao == "3":
            caminho = criar_arquivo_exemplo()
            print(f"‚úÖ Arquivo de exemplo criado: {caminho}")
        
        elif opcao == "4":
            testar_tokens_individuais()
        
        elif opcao == "5":
            print("üëã Saindo do analisador l√©xico. At√© logo!")
            break
        
        else:
            print("‚ùå Op√ß√£o inv√°lida. Tente novamente.")


def main():
    """Fun√ß√£o principal do sistema de teste"""
    print("üöÄ SISTEMA DE TESTE DE AN√ÅLISE L√âXICA")
    print("Baseado na linguagem especificada em github.com/Raul-Mozart/compiladores")
    print("Integrado com os aut√¥matos DFA existentes")
    
    # Executa menu interativo
    menu_interativo()


if __name__ == "__main__":
    main()