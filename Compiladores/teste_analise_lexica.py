"""
SISTEMA DE TESTE DE AN√ÅLISE L√âXICA E SEM√ÇNTICA
Integrado com os aut√¥matos existentes para testar a linguagem proposta

Este m√≥dulo permite:
1. Testar tokens individuais usando os aut√¥matos DFA
2. Processar arquivos de c√≥digo na linguagem proposta
3. Avaliar input do usu√°rio interativamente
4. Gerar relat√≥rios detalhados de an√°lise l√©xica
5. Validar compatibilidade sem√¢ntica de tipos (an√°lise sem√¢ntica b√°sica)
"""

import re
import os
from typing import List, Dict, Tuple, Optional, NamedTuple
from automatos import compile_regex_to_dfa
from automatos.constantes import TOKENS_COMUNS


class ErroSemantico(NamedTuple):
    """Representa um erro sem√¢ntico encontrado durante a an√°lise"""
    tipo: str
    mensagem: str
    linha: int
    coluna: int
    contexto: str


class ValidadorSemantico:
    """Validador sem√¢ntico b√°sico para verificar compatibilidade de tipos"""
    
    def __init__(self):
        """Inicializa o validador sem√¢ntico"""
        self.tipos_validos = {"int", "float", "string", "bool", "list"}
        self.erros_semanticos = []
    
    def limpar_erros(self):
        """Limpa a lista de erros sem√¢nticos"""
        self.erros_semanticos = []
    
    def eh_literal_valido_para_tipo(self, tipo: str, token_type: str, lexeme: str) -> bool:
        """
        Verifica se um literal √© compat√≠vel com um tipo declarado
        
        Args:
            tipo: tipo declarado (int, float, string, bool, list)
            token_type: tipo do token encontrado
            lexeme: valor do token
            
        Returns:
            True se compat√≠vel, False caso contr√°rio
        """
        if tipo == "int":
            return token_type == "INT_LITERAL"
        elif tipo == "float":
            return token_type in ["FLOAT_LITERAL", "INT_LITERAL"]  # int pode ser promovido para float
        elif tipo == "string":
            return token_type == "STRING_LITERAL"
        elif tipo == "bool":
            return token_type == "BOOL_LITERAL"
        elif tipo == "list":
            # Para lista, aceitamos arrays literais [1,2,3] - simplificado
            return lexeme.startswith("[") and lexeme.endswith("]")
        
        return False
    
    def validar_declaracao_variavel(self, tokens: List[Tuple[str, str, int, int]], posicao: int) -> Optional[ErroSemantico]:
        """
        Valida uma declara√ß√£o de vari√°vel: var Type identificador as valor
        
        Args:
            tokens: lista completa de tokens
            posicao: posi√ß√£o do token 'var'
            
        Returns:
            ErroSemantico se encontrar problema, None caso contr√°rio
        """
        try:
            # Padr√£o esperado: var Type identificador as valor
            if posicao + 4 >= len(tokens):
                return ErroSemantico(
                    "DECLARACAO_INCOMPLETA",
                    "Declara√ß√£o de vari√°vel incompleta",
                    tokens[posicao][2],
                    tokens[posicao][3],
                    "var ... (incompleta)"
                )
            
            var_token = tokens[posicao]
            type_token = tokens[posicao + 1]
            identifier_token = tokens[posicao + 2]
            as_token = tokens[posicao + 3]
            value_token = tokens[posicao + 4]
            
            # Verifica se segue o padr√£o b√°sico
            if (var_token[0] != "KEYWORD" or var_token[1] != "var" or
                type_token[0] != "KEYWORD" or type_token[1] not in self.tipos_validos or
                identifier_token[0] != "IDENTIFIER" or
                as_token[0] != "KEYWORD" or as_token[1] != "as"):
                return None  # N√£o √© uma declara√ß√£o de vari√°vel v√°lida
            
            # Verifica compatibilidade de tipos
            tipo_declarado = type_token[1]
            valor_token_type = value_token[0]
            valor_lexeme = value_token[1]
            
            if not self.eh_literal_valido_para_tipo(tipo_declarado, valor_token_type, valor_lexeme):
                contexto = f"var {tipo_declarado} {identifier_token[1]} as {valor_lexeme}"
                
                # Mensagem espec√≠fica baseada no tipo de erro
                if valor_token_type == "IDENTIFIER":
                    mensagem = f"Tipo incompat√≠vel: vari√°vel '{identifier_token[1]}' declarada como '{tipo_declarado}' mas recebeu identificador '{valor_lexeme}' (esperado literal {tipo_declarado})"
                else:
                    mensagem = f"Tipo incompat√≠vel: vari√°vel '{identifier_token[1]}' declarada como '{tipo_declarado}' mas recebeu {valor_token_type} '{valor_lexeme}'"
                
                return ErroSemantico(
                    "TIPO_INCOMPATIVEL",
                    mensagem,
                    value_token[2],
                    value_token[3],
                    contexto
                )
            
            return None  # Tudo OK
            
        except Exception as e:
            return ErroSemantico(
                "ERRO_VALIDACAO",
                f"Erro durante valida√ß√£o: {e}",
                tokens[posicao][2] if posicao < len(tokens) else 0,
                tokens[posicao][3] if posicao < len(tokens) else 0,
                "erro interno"
            )
    
    def validar_tokens(self, tokens: List[Tuple[str, str, int, int]]) -> List[ErroSemantico]:
        """
        Valida a lista completa de tokens para erros sem√¢nticos
        
        Args:
            tokens: lista de tokens (token_type, lexeme, linha, coluna)
            
        Returns:
            Lista de erros sem√¢nticos encontrados
        """
        self.limpar_erros()
        
        i = 0
        while i < len(tokens):
            token_type, lexeme, linha, coluna = tokens[i]
            
            # Procura por declara√ß√µes de vari√°veis (var)
            if token_type == "KEYWORD" and lexeme == "var":
                erro = self.validar_declaracao_variavel(tokens, i)
                if erro:
                    self.erros_semanticos.append(erro)
                # Pula a declara√ß√£o completa
                i += 5  # var Type id as value
            else:
                i += 1
        
        return self.erros_semanticos


class AnalisadorLexico:
    """Analisador l√©xico e sem√¢ntico para a linguagem proposta"""
    
    def __init__(self):
        """Inicializa o analisador com tokens da linguagem proposta"""
        # Tokens espec√≠ficos da linguagem (baseados na especifica√ß√£o GitHub)
        self.tokens_linguagem = {
            # Palavras-chave (incluindo 'as' que estava faltando)
            "KEYWORD": r"(if|else|for|while|function|var|in|class|return|string|int|float|bool|list|and|or|not|private|public|mutable|inherits|new|as)",
            
            # Identificadores (mais espec√≠fico que o gen√©rico)
            "IDENTIFIER": r"[A-Za-z_][A-Za-z0-9_]*",
            
            # Literais num√©ricos
            "INT_LITERAL": r"\d+",
            "FLOAT_LITERAL": r"\d+\.\d+([eE][+-]?\d+)?",
            
            # Literais de string (com escape)
            "STRING_LITERAL": r'"[^"]*"',
            
            # Literais booleanos
            "BOOL_LITERAL": r"(true|false)",
            
            # Operadores relacionais
            "RELOP": r"(==|!=|<=|>=|<|>)",
            
            # Operadores aritm√©ticos
            "ARITHOP": r"[+\-*/%]",
            
            # Operador de atribui√ß√£o
            "ASSIGN": r"=",
            
            # Delimitadores
            "LPAREN": r"\(",
            "RPAREN": r"\)",
            "LBRACE": r"\{",
            "RBRACE": r"\}",
            "LBRACKET": r"\[",
            "RBRACKET": r"\]",
            
            # Pontua√ß√£o
            "SEMICOLON": r";",
            "COMMA": r",",
            "DOT": r"\.",
            "COLON": r":",
            
            # Quebra de linha e espa√ßos
            "NEWLINE": r"\n",
            "WHITESPACE": r"[ \t\r]+",
            
            # Coment√°rios (adicionando suporte)
            "COMMENT": r"//[^\n]*",
            
            # Operador de range
            "RANGE": r"\.\."
        }
        
        # Compilar todos os tokens para DFA
        self.dfas_compilados = {}
        self.validador_semantico = ValidadorSemantico()
        self.compilar_tokens()
    
    def compilar_tokens(self):
        """Compila todos os tokens para aut√¥matos DFA"""
        print("üîÑ Compilando tokens para aut√¥matos DFA...")
        
        for nome, regex in self.tokens_linguagem.items():
            try:
                dfa = compile_regex_to_dfa(regex)
                self.dfas_compilados[nome] = dfa
                print(f"  ‚úÖ {nome}: {len(dfa.states)} estados")
            except Exception as e:
                print(f"  ‚ùå {nome}: ERRO ‚Üí {e}")
        
        print(f"üìä Total de tokens compilados: {len(self.dfas_compilados)}")
    
    def tokenizar_codigo(self, codigo: str) -> List[Tuple[str, str, int, int]]:
        """
        Tokeniza c√≥digo fonte da linguagem
        
        Returns:
            Lista de (token_type, lexeme, linha, coluna)
        """
        tokens_encontrados = []
        linha_atual = 1
        coluna_atual = 1
        posicao = 0
        
        # Ordem de prioridade para tokens (mais espec√≠ficos primeiro)
        ordem_prioridade = [
            "KEYWORD", "BOOL_LITERAL", "FLOAT_LITERAL", "INT_LITERAL", 
            "STRING_LITERAL", "RELOP", "ARITHOP", "ASSIGN", "RANGE",
            "LPAREN", "RPAREN", "LBRACE", "RBRACE", "LBRACKET", "RBRACKET",
            "SEMICOLON", "COMMA", "DOT", "COLON", "COMMENT",
            "IDENTIFIER", "NEWLINE", "WHITESPACE"
        ]
        
        while posicao < len(codigo):
            token_encontrado = False
            
            # Tenta encontrar o melhor match com base na prioridade
            melhor_match = None
            maior_comprimento = 0
            
            # Testa tokens na ordem de prioridade
            for nome_token in ordem_prioridade:
                if nome_token not in self.dfas_compilados:
                    continue
                    
                dfa = self.dfas_compilados[nome_token]
                
                # Testa substring a partir da posi√ß√£o atual
                # N√ÉO para na primeira rejei√ß√£o - continua testando
                for fim in range(posicao + 1, len(codigo) + 1):
                    substring = codigo[posicao:fim]
                    
                    if dfa.accepts(substring):
                        # Se encontrou match maior ou de mesma prioridade, atualiza
                        if len(substring) > maior_comprimento:
                            maior_comprimento = len(substring)
                            melhor_match = (nome_token, substring)
                    # REMOVIDO: else break - isso estava causando o problema!
                
                # Se j√° encontrou um token de alta prioridade, para
                if melhor_match and nome_token in ["KEYWORD", "BOOL_LITERAL", "FLOAT_LITERAL", "INT_LITERAL", "STRING_LITERAL"]:
                    break
            
            if melhor_match:
                token_type, lexeme = melhor_match
                
                # P√≥s-processamento: verifica se IDENTIFIER √© na verdade uma KEYWORD
                if token_type == "IDENTIFIER" and lexeme in ["if", "else", "for", "while", "function", "var", "in", "class", "return", "string", "int", "float", "bool", "list", "and", "or", "not", "private", "public", "mutable", "inherits", "new", "as"]:
                    token_type = "KEYWORD"
                
                # Pula whitespace e coment√°rios na sa√≠da (mas ainda processa)
                if token_type not in ["WHITESPACE", "COMMENT"]:
                    tokens_encontrados.append((token_type, lexeme, linha_atual, coluna_atual))
                
                # Atualiza posi√ß√£o
                posicao += len(lexeme)
                
                # Atualiza linha e coluna
                if token_type == "NEWLINE" or '\n' in lexeme:
                    linha_atual += lexeme.count('\n')
                    coluna_atual = 1
                else:
                    coluna_atual += len(lexeme)
                
                token_encontrado = True
            
            if not token_encontrado:
                # Caractere n√£o reconhecido
                char = codigo[posicao]
                tokens_encontrados.append(("UNKNOWN", char, linha_atual, coluna_atual))
                posicao += 1
                coluna_atual += 1
        
        return tokens_encontrados
    
    def validar_token_individual(self, token_type: str, lexeme: str) -> bool:
        """Valida se um lexeme corresponde a um tipo de token espec√≠fico"""
        if token_type in self.dfas_compilados:
            return self.dfas_compilados[token_type].accepts(lexeme)
        return False
    
    def processar_codigo_completo(self, codigo: str) -> Dict:
        """
        Processa c√≥digo fonte com an√°lise l√©xica E sem√¢ntica
        
        Args:
            codigo: c√≥digo fonte a ser analisado
            
        Returns:
            Dicion√°rio com tokens, erros l√©xicos, erros sem√¢nticos e estat√≠sticas
        """
        # An√°lise l√©xica
        tokens = self.tokenizar_codigo(codigo)
        
        # Separar tokens v√°lidos dos inv√°lidos
        tokens_validos = []
        erros_lexicos = []
        
        for token_type, lexeme, linha, coluna in tokens:
            if token_type == "UNKNOWN":
                erros_lexicos.append({
                    "tipo": "CARACTERE_INVALIDO",
                    "mensagem": f"Caractere inv√°lido: '{lexeme}'",
                    "linha": linha,
                    "coluna": coluna,
                    "contexto": lexeme
                })
            else:
                tokens_validos.append((token_type, lexeme, linha, coluna))
        
        # An√°lise sem√¢ntica (apenas se n√£o h√° erros l√©xicos cr√≠ticos)
        erros_semanticos = []
        if not erros_lexicos:
            erros_semanticos = self.validador_semantico.validar_tokens(tokens_validos)
        
        # Estat√≠sticas
        stats_tokens = {}
        for token_type, _, _, _ in tokens_validos:
            stats_tokens[token_type] = stats_tokens.get(token_type, 0) + 1
        
        return {
            "tokens": tokens_validos,
            "erros_lexicos": erros_lexicos,
            "erros_semanticos": [erro._asdict() for erro in erros_semanticos],
            "estatisticas_tokens": stats_tokens,
            "total_tokens": len(tokens_validos),
            "total_erros": len(erros_lexicos) + len(erros_semanticos),
            "linhas_codigo": codigo.count('\n') + 1,
            "analise_ok": len(erros_lexicos) == 0 and len(erros_semanticos) == 0
        }
    def processar_arquivo(self, caminho_arquivo: str) -> Dict:
        """Processa arquivo de c√≥digo e retorna an√°lise completa (l√©xica + sem√¢ntica)"""
        if not os.path.exists(caminho_arquivo):
            return {"erro": f"Arquivo n√£o encontrado: {caminho_arquivo}"}
        
        try:
            with open(caminho_arquivo, 'r', encoding='utf-8') as arquivo:
                codigo = arquivo.read()
            
            resultado = self.processar_codigo_completo(codigo)
            resultado["arquivo"] = caminho_arquivo
            
            return resultado
        
        except Exception as e:
            return {"erro": f"Erro ao processar arquivo: {e}"}
    
    def gerar_relatorio(self, resultado: Dict) -> str:
        """Gera relat√≥rio detalhado da an√°lise l√©xica e sem√¢ntica"""
        if "erro" in resultado:
            return f"‚ùå ERRO: {resultado['erro']}"
        
        relatorio = []
        relatorio.append("=" * 80)
        relatorio.append("üìã RELAT√ìRIO DE AN√ÅLISE L√âXICA E SEM√ÇNTICA")
        relatorio.append("=" * 80)
        
        # Informa√ß√µes b√°sicas
        if "arquivo" in resultado:
            relatorio.append(f"üìÅ Arquivo: {resultado['arquivo']}")
        relatorio.append(f"üìä Total de tokens: {resultado['total_tokens']}")
        relatorio.append(f"üìù Linhas de c√≥digo: {resultado['linhas_codigo']}")
        relatorio.append(f"üîç Status da an√°lise: {'‚úÖ OK' if resultado.get('analise_ok', False) else '‚ùå ERROS ENCONTRADOS'}")
        relatorio.append("")
        
        # Erros l√©xicos
        erros_lexicos = resultado.get('erros_lexicos', [])
        if erros_lexicos:
            relatorio.append("üö´ ERROS L√âXICOS ENCONTRADOS:")
            relatorio.append("-" * 50)
            for i, erro in enumerate(erros_lexicos, 1):
                relatorio.append(f"  {i}. [{erro['linha']}:{erro['coluna']}] {erro['tipo']}")
                relatorio.append(f"     {erro['mensagem']}")
                relatorio.append(f"     Contexto: {erro['contexto']}")
                relatorio.append("")
        
        # Erros sem√¢nticos
        erros_semanticos = resultado.get('erros_semanticos', [])
        if erros_semanticos:
            relatorio.append("‚ö†Ô∏è  ERROS SEM√ÇNTICOS ENCONTRADOS:")
            relatorio.append("-" * 50)
            for i, erro in enumerate(erros_semanticos, 1):
                relatorio.append(f"  {i}. [{erro['linha']}:{erro['coluna']}] {erro['tipo']}")
                relatorio.append(f"     {erro['mensagem']}")
                relatorio.append(f"     Contexto: {erro['contexto']}")
                relatorio.append("")
        
        # Estat√≠sticas por tipo (apenas se n√£o h√° erros)
        if resultado.get('analise_ok', False) or not (erros_lexicos or erros_semanticos):
            relatorio.append("üìà ESTAT√çSTICAS POR TIPO DE TOKEN:")
            relatorio.append("-" * 50)
            stats = resultado.get('estatisticas_tokens', resultado.get('estatisticas', {}))
            for token_type, count in sorted(stats.items()):
                relatorio.append(f"  {token_type:15}: {count:4d} ocorr√™ncias")
            relatorio.append("")
        
        # Todos os tokens encontrados (apenas se an√°lise OK)
        if resultado.get('analise_ok', False):
            relatorio.append("üîç TOKENS ENCONTRADOS:")
            relatorio.append("-" * 50)
            relatorio.append(f"{'LINHA':>5} {'COL':>4} {'TIPO':15} {'LEXEME'}")
            relatorio.append("-" * 50)
            
            for token_type, lexeme, linha, coluna in resultado['tokens']:
                # Escape strings para display
                if token_type == "STRING_LITERAL":
                    lexeme_display = repr(lexeme)
                else:
                    lexeme_display = lexeme.replace('\n', '\\n').replace('\t', '\\t')
                
                relatorio.append(f"{linha:5d} {coluna:4d} {token_type:15} {lexeme_display}")
        
        return "\n".join(relatorio)


def testar_tokens_individuais():
    """Testa tokens individuais usando os aut√¥matos"""
    print("\n" + "=" * 60)
    print("üß™ TESTANDO TOKENS INDIVIDUAIS")
    print("=" * 60)
    
    analisador = AnalisadorLexico()
    
    casos_teste = {
        "KEYWORD": ["if", "else", "function", "var", "while", "for", "invalid_keyword"],
        "IDENTIFIER": ["variavel", "_private", "contador1", "9invalid", "valid_name"],
        "INT_LITERAL": ["123", "0", "007", "12.5", "abc"],
        "FLOAT_LITERAL": ["3.14", "0.0", "1.5e10", "123", ".5"],
        "STRING_LITERAL": ['"hello"', '"world \\"quoted\\""', '""', '"unterminated', '"valid"'],
        "BOOL_LITERAL": ["true", "false", "True", "FALSE", "maybe"],
        "RELOP": ["==", "!=", "<=", ">=", "<", ">", "==="],
        "ARITHOP": ["+", "-", "*", "/", "%", "++", "--"],
    }
    
    for token_type, exemplos in casos_teste.items():
        print(f"\nüè∑Ô∏è  {token_type}:")
        for exemplo in exemplos:
            valido = analisador.validar_token_individual(token_type, exemplo)
            status = "‚úÖ" if valido else "‚ùå"
            print(f"  {status} '{exemplo}'")


def criar_arquivo_exemplo():
    """Cria arquivo de exemplo com c√≥digo na linguagem proposta"""
    codigo_exemplo = '''// PROGRAMA DE EXEMPLO - Linguagem Proposta
// Testando todos os tipos de tokens

var int idade as 25;
var float altura as 1.75;
var string nome as "Jo√£o Silva";
var bool ativo as true;
var list numeros as [1, 2, 3, 4, 5];

function string verificarIdade(idade) {
    if (idade >= 18) {
        return "Maior de idade";
    } else if (idade >= 13) {
        return "Adolescente";  
    } else {
        return "Crian√ßa";
    }
}

function float calcularMedia(a, b, c) {
    var float soma as a + b + c;
    var float media as soma / 3.0;
    return media;
}

class Pessoa {
    var string nome as "";
    var int idade as 0;
    
    function void setNome(novoNome) {
        nome = novoNome;
    }
}
return

// Programa principal
var int numeroA as 10;
var int numeroB as 20; 
var float resultado as calcularMedia(numeroA, numeroB, 15);

if (resultado > 15.0 and numeroA != numeroB) {
    print("Resultado v√°lido: " + resultado);
} else {
    print("Resultado inv√°lido");
}

for int i in 1 .. 5 {
    print("N√∫mero: " + i);
}
'''
    
    caminho = "codigo_exemplo.txt"
    with open(caminho, 'w', encoding='utf-8') as arquivo:
        arquivo.write(codigo_exemplo)
    
    return caminho


def menu_interativo():
    """Menu interativo para o usu√°rio testar o analisador"""
    analisador = AnalisadorLexico()
    
    while True:
        print("\n" + "=" * 60)
        print("üî¨ ANALISADOR L√âXICO E SEM√ÇNTICO - MENU INTERATIVO")
        print("=" * 60)
        print("1. Testar arquivo de c√≥digo")
        print("2. Testar c√≥digo digitado")
        print("3. Criar arquivo de exemplo")
        print("4. Testar tokens individuais")
        print("5. Testar declara√ß√µes espec√≠ficas")
        print("6. Sair")
        
        opcao = input("\nüìù Escolha uma op√ß√£o (1-6): ").strip()
        
        if opcao == "1":
            caminho = input("üìÅ Digite o caminho do arquivo: ").strip()
            if not caminho:
                caminho = "codigo_exemplo.txt"
                print(f"üìã Usando arquivo padr√£o: {caminho}")
            
            resultado = analisador.processar_arquivo(caminho)
            relatorio = analisador.gerar_relatorio(resultado)
            print("\n" + relatorio)
            
            # Salvar relat√≥rio
            with open("relatorio_analise.txt", "w", encoding="utf-8") as f:
                f.write(relatorio)
            print(f"\nüíæ Relat√≥rio salvo em: relatorio_analise.txt")
        
        elif opcao == "2":
            print("üìù Digite o c√≥digo (termine com linha vazia):")
            linhas = []
            while True:
                linha = input()
                if linha.strip() == "":
                    break
                linhas.append(linha)
            
            codigo = "\n".join(linhas)
            if codigo.strip():
                resultado = analisador.processar_codigo_completo(codigo)
                relatorio = analisador.gerar_relatorio(resultado)
                print("\n" + relatorio)
        
        elif opcao == "3":
            caminho = criar_arquivo_exemplo()
            print(f"‚úÖ Arquivo de exemplo criado: {caminho}")
        
        elif opcao == "4":
            testar_tokens_individuais()
        
        elif opcao == "5":
            testar_declaracoes_especificas(analisador)
        
        elif opcao == "6":
            print("üëã Saindo do analisador. At√© logo!")
            break
        
        else:
            print("‚ùå Op√ß√£o inv√°lida. Tente novamente.")


def testar_declaracoes_especificas(analisador: AnalisadorLexico):
    """Testa declara√ß√µes espec√≠ficas para valida√ß√£o sem√¢ntica"""
    print("\n" + "=" * 60)
    print("üß™ TESTE DE DECLARA√á√ïES DE VARI√ÅVEIS")
    print("=" * 60)
    
    casos_teste = [
        # Casos v√°lidos
        ("var int idade as 25;", "‚úÖ V√ÅLIDO"),
        ("var float altura as 1.75;", "‚úÖ V√ÅLIDO"),
        ("var string nome as \"Jo√£o\";", "‚úÖ V√ÅLIDO"),
        ("var bool ativo as true;", "‚úÖ V√ÅLIDO"),
        ("var int numero as 42;", "‚úÖ V√ÅLIDO"),
        
        # Casos inv√°lidos (o que voc√™ quer que d√™ erro)
        ("var int idade as b;", "‚ùå ERRO ESPERADO"),
        ("var float altura as nome;", "‚ùå ERRO ESPERADO"),
        ("var string texto as 123;", "‚ùå ERRO ESPERADO"),
        ("var bool flag as \"false\";", "‚ùå ERRO ESPERADO"),
        ("var int valor as 3.14;", "‚ùå ERRO ESPERADO"),
    ]
    
    for codigo, expectativa in casos_teste:
        print(f"\nüîç Testando: {codigo}")
        print(f"   Expectativa: {expectativa}")
        
        resultado = analisador.processar_codigo_completo(codigo)
        
        if resultado['erros_semanticos']:
            print("   ÔøΩ Resultado: ERRO SEM√ÇNTICO encontrado")
            for erro in resultado['erros_semanticos']:
                print(f"      ‚Ä¢ {erro['mensagem']}")
        elif resultado['erros_lexicos']:
            print("   üö´ Resultado: ERRO L√âXICO encontrado")
            for erro in resultado['erros_lexicos']:
                print(f"      ‚Ä¢ {erro['mensagem']}")
        else:
            print("   ‚úÖ Resultado: An√°lise OK")
        
        print("   " + "-" * 50)


def main():
    """Fun√ß√£o principal do sistema de teste"""
    print("üöÄ SISTEMA DE TESTE DE AN√ÅLISE L√âXICA E SEM√ÇNTICA")
    print("Baseado na linguagem especificada em github.com/Raul-Mozart/compiladores")
    print("Integrado com os aut√¥matos DFA existentes")
    print("‚ú® Agora com valida√ß√£o sem√¢ntica de tipos!")
    
    # Executa menu interativo
    menu_interativo()


if __name__ == "__main__":
    main()